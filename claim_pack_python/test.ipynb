{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "0:00:00.062831\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import snowflake.connector as sf\n",
    "import os\n",
    "import xlwings as xw\n",
    "from xlwings.constants import DeleteShiftDirection\n",
    "import datetime\n",
    "from pywintypes import com_error\n",
    "import win32com.client as win32\n",
    "import logging\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# current_dir = os.getcwd()\n",
    "# os.chdir(r'D:\\python\\claim_pack_python')\n",
    "os.chdir(\"D:\\\\python\\\\claim_pack_python\")\n",
    "current_dir = r'D:\\python\\claim_pack_python'\n",
    "###### Analyst fill\n",
    "folder_name = '202203'\n",
    "month_filter = '202203'\n",
    "###############################################\n",
    "config_coles = r\"config.json\"\n",
    "\n",
    "file_sql_claimpack = r\"claim_pack_state.sql\"\n",
    "file_sql_summ = r\"summarizer.sql\"\n",
    "file_sql_summ_state = r\"summarizer_state.sql\"\n",
    "file_sql_summ_th = r\"summarizer_th.sql\"\n",
    "file_sql_summ_state_th = r\"summarizer_state_th.sql\"\n",
    "file_sql_cd_ref = r\"cd_ref.sql\"\n",
    "file_sql_cd_check_cole_online = r\"cd_check_cole_online.sql\"\n",
    "file_sql_ven_stop_trading = r\"check_ven_stop_trading.sql\"\n",
    "file_sql_cd_check_prgx = r\"cd_check_prgx.sql\"\n",
    "file_sql_cd_ref_listagg = r\"cd_ref_listagg.sql\"\n",
    "file_sql_cd_ref_listagg_item = r\"cd_ref_listagg_item.sql\"\n",
    "file_sql_check_prof = r\"check_profectus_detail.sql\"\n",
    "\n",
    "path_check_list = fr\"D:\\\\python\\\\claim_pack_python\\\\claim_qty\\\\{folder_name}\\\\checklist.csv\"\n",
    "path_check_list_promo = fr\"D:\\\\python\\\\claim_pack_python\\\\claim_qty\\\\{folder_name}\\\\check_list_promo.xlsx\"\n",
    "\n",
    "path_export = fr\"D:\\\\python\\\\claim_pack_python\\\\claim_qty\\\\{folder_name}\\\\\"\n",
    "path_excel = r\"CS_SCAN_Vendorname_Analyst_Date.xlsx\"\n",
    "path_dna = r\"DNA.xlsx\"\n",
    "\n",
    "iconPath_email = r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\OUTLOOK.EXE\"\n",
    "iconPath_excel = r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"\n",
    "\n",
    "try:\n",
    "    os.remove(r'D:\\python\\claim_pack_python\\claim_pack_scan.log')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(filename=\"claim_pack_scan.log\",\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    filemode='a')\n",
    " \n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    " \n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def set_up(config):\n",
    "    \"\"\"Set up connection to SnowFlake\"\"\"\n",
    "    config = json.loads(open(config).read())\n",
    "    account = config['snowflake']['account']\n",
    "    user = config['snowflake']['user']\n",
    "    warehouse = config['snowflake']['warehouse']\n",
    "    role = config['snowflake']['role']\n",
    "    database = config['snowflake']['database']\n",
    "    schema = config['snowflake']['schema']\n",
    "    password = config['snowflake']['password']\n",
    "    auth = config['snowflake']['authenticator']\n",
    "\n",
    "    conn = sf.connect(user=user, password=password, account=account, authenticator=auth,\n",
    "                      warehouse=warehouse, role=role, database=database, schema=schema)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    return cursor\n",
    "def connect_sql(cursor,file_sql,item_code=0,var_1=0,var_2=0,var_3=0,var_4=0,var_5=''):\n",
    "    try:\n",
    "        # cursor.execute((open(file_sql).read()))\n",
    "        cursor.execute((open(file_sql).read()).format(item_code,var_1,var_2,var_3,var_4,var_5))\n",
    "        all_rows = cursor.fetchall()\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "    finally:\n",
    "        pass\n",
    "        # conn.close()\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    try:\n",
    "        df.columns = field_names\n",
    "    except ValueError:\n",
    "        return pd.DataFrame(columns= field_names)\n",
    "    return df\n",
    "\n",
    "def convert_to_input_sql(num_list):\n",
    "    num_list_final = ''\n",
    "    # print('SUPP LIST',supp_num_list)\n",
    "    for num_list in num_list:\n",
    "        num_list_final = num_list_final + \"'\" + num_list + \"',\"\n",
    "    return num_list_final[:-1]\n",
    "\n",
    "def convert_to_input_function(num_list):\n",
    "    num_list_final = ''\n",
    "    # print('SUPP LIST',supp_num_list)\n",
    "    for num_list in num_list:\n",
    "        num_list_final = num_list_final + num_list + ','\n",
    "    return num_list_final[:-1]\n",
    "\n",
    "def get_info(df_splited):\n",
    "    supp_num_list = list(df_splited['SUPPLIER'].drop_duplicates())\n",
    "    item_list = list(df_splited['SKU_ID'].drop_duplicates())\n",
    "\n",
    "    supp_num_list_final = convert_to_input_sql(num_list = supp_num_list)\n",
    "    item_list_final = convert_to_input_sql(num_list = item_list)\n",
    "    item_input_function = convert_to_input_function(num_list = item_list)\n",
    "    return supp_num_list_final,item_list_final,item_input_function\n",
    "\n",
    "def writer_excel(data,remove,number_sheet,path_export_final):\n",
    "    # data = list_data, remove = list_remove,number_sheet= str(index_promo)+'_'+str(gst),path_export_final=path_export_final\n",
    "    #select sheet\n",
    "    sheet_df_mapping = {number_sheet: data}\n",
    "    sheet_df_remove  = {number_sheet: remove}\n",
    "    # Open Excel in background\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb = app.books.open(path_export_final)\n",
    "        # List of current worksheet names\n",
    "        current_sheets = [sheet.name for sheet in wb.sheets]\n",
    "        # Iterate over sheet/df mapping\n",
    "        # If sheet already exist, overwrite current cotent. Else, add new sheet\n",
    "        for sheet_name in sheet_df_mapping.keys():\n",
    "            if sheet_name in current_sheets:\n",
    "                for df_data in data :\n",
    "                    wb.sheets(sheet_name).range(df_data['cell_export']).options(index=False,header=False).value = df_data['df']\n",
    "            else:\n",
    "                'Name of sheet cannot be found in Excel file, please check again'\n",
    "        for sheet_name in sheet_df_remove.keys():\n",
    "            if sheet_name in current_sheets:\n",
    "                for df_remove in remove :\n",
    "                    # wb.sheets(sheet_name).range(df_cell['cell_export']).options(index=False,header=False).value = df_cell['df']\n",
    "                    length_start = df_remove['length_start'] + df_remove['count_df']\n",
    "                    range_length_to_remove = str(length_start)+':'+ str(df_remove['length_end'])\n",
    "                    wb.sheets(sheet_name).range(range_length_to_remove).api.Delete(DeleteShiftDirection.xlShiftUp)\n",
    "            else:\n",
    "                'Name of sheet cannot be found in Excel file, please check again'\n",
    "        wb.save(path_export_final)\n",
    "    return None\n",
    "\n",
    "def fill_summary_sheet(summary_index_list,path_export_final):\n",
    "    print('Start fill summary sheet')\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb_from = app.books.open(path_export_final)\n",
    "        summary_index = 1\n",
    "        for index in summary_index_list:\n",
    "            wb_from.sheets['Vendor Summary'].range('B'+str(summary_index+10)).value = index\n",
    "            summary_index += 1\n",
    "        length_start = summary_index + 10\n",
    "        range_length_to_remove = str(length_start)+':'+ str(30)\n",
    "        print(range_length_to_remove)\n",
    "        wb_from.sheets('Vendor Summary').range(range_length_to_remove).api.Delete(DeleteShiftDirection.xlShiftUp)         \n",
    "        wb_from.save(path_export_final)\n",
    "    return 'Done fill summary sheet' \n",
    "\n",
    "def create_worksheet(index_promo,gst,path_export_final):\n",
    "    # Open Excel in background\n",
    "    with xw.App(visible=False) as app:\n",
    "        if index_promo == 1:\n",
    "            wb_from = app.books.open(path_excel)\n",
    "        else :\n",
    "            wb_from = app.books.open(path_export_final)\n",
    "        ws_from = wb_from.sheets['template']\n",
    "        ws_from.copy(before=ws_from, name=str(index_promo)+'_'+str(gst))\n",
    "        wb_from.save(path_export_final)\n",
    "    return 'Done create worksheet'     \n",
    "\n",
    "def remove_sheet_change_xlsb(sheet_name,path_export_final,path_export_final_xlsb):\n",
    "    print('Start delete sheet & change to xlsb')\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb = app.books.open(path_export_final)                \n",
    "        wb.sheets[sheet_name].delete()\n",
    "        wb.save(path_export_final_xlsb)\n",
    "    try:\n",
    "        os.remove(path_export_final)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return print('Done delete sheet & change to xlsb')\n",
    "\n",
    "# item_code=0,var_1=0,var_2=0,var_3=0,var_4=0\n",
    "\n",
    "def df_sales_data(cursor, item_list_dict_gsted,start_date,end_date,classify_state):\n",
    "    i = 0\n",
    "    print(classify_state)\n",
    "    # if classify_state == 'TO QA_STATE_SP' or  classify_state == 'TO QA_STATE_TH' :\n",
    "    if 'STATE' in classify_state :\n",
    "        for key,value in item_list_dict_gsted.items():\n",
    "            print(key,value)\n",
    "            item_code,state = key\n",
    "            # if classify_state == 'TO QA_STATE_SP':\n",
    "            if '_SP' in classify_state :\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_state ,item_code = item_code,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1],var_5 = state)\n",
    "            else:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_state_th ,item_code = item_code,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1],var_5 = state)\n",
    "            if i == 0:\n",
    "                df_merge = df_each_item\n",
    "            else :\n",
    "                df_merge = pd.concat([df_merge, df_each_item], ignore_index=True)\n",
    "            i+=1\n",
    "        df_merge['ELI_CLAIM'] = df_merge.RQTY_PROMO * df_merge.SCAN_RATE\n",
    "        df_merge= df_merge.sort_values(by=['RSKU_ID','RDAY_DT','RSTATE'], ascending=True).reset_index(drop=True)\n",
    "    else:\n",
    "        for key,value in item_list_dict_gsted.items():\n",
    "            print(key,value)\n",
    "            # if classify_state == 'TO QA_NATIONAL_SP':\n",
    "            if '_SP' in classify_state:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ ,item_code = key,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1])\n",
    "            else:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_th ,item_code = key,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1])\n",
    "            if i == 0:\n",
    "                df_merge = df_each_item\n",
    "            else :\n",
    "                df_merge = pd.concat([df_merge, df_each_item], ignore_index=True)\n",
    "            i+=1\n",
    "        df_merge['ELI_CLAIM'] = df_merge.RQTY_PROMO * df_merge.SCAN_RATE\n",
    "        df_merge= df_merge.sort_values(by=['RSKU_ID','RDAY_DT','RSTATE'], ascending=True).reset_index(drop=True)\n",
    "    return df_merge\n",
    "\n",
    "def product_state_summary(df,df_ref):\n",
    "    print('Start product_state_summary')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    # Find distict item_code and state\n",
    "    # writer_excel(df,cell_export,length_start,count_df,length_end,number_sheet,path_export_final)\n",
    "    df_temp =df.drop_duplicates(['RSKU_ID','RITEM_DESC','RSTATE'])[['RSKU_ID','RITEM_DESC','RSTATE']]\n",
    "    df_temp_2 = pd.merge(df_temp,df_ref,left_on=['RSKU_ID','RSTATE'],right_on=['ITEM_IDNT','CLM_STATE'], how='left')\n",
    "    df_final = df_temp_2[['RSKU_ID','RITEM_DESC','RSTATE','REF_NUM','CLM_QTY','CLM_RATE','CLM_PRODUCT']]\n",
    "    df_sku_desc = df_final[['RSKU_ID','RITEM_DESC']]\n",
    "    df_state = df_final[['RSTATE']]\n",
    "    df_ref = df_final[['REF_NUM','CLM_QTY','CLM_RATE']]\n",
    "    df_ref.insert(1,\"REF_DESC\",'')\n",
    "    # Calculate number of rows\n",
    "    number_rows_state = len(df_ref)\n",
    "\n",
    "    dict_data_sku = {'df':df_sku_desc,'cell_export':'B121'}\n",
    "    dict_data_state = {'df':df_state,'cell_export':'E121'}\n",
    "    dict_data_remove = {'df':df_ref,'cell_export':'M121'}\n",
    "    dict_remove = {'count_df':number_rows_state,'length_start':121,'length_end':601}\n",
    "    list_data.append(dict_data_sku)\n",
    "    list_data.append(dict_data_state)\n",
    "    list_data.append(dict_data_remove)\n",
    "    list_remove.append(dict_remove)\n",
    "    print('Done product_state_summary')\n",
    "    return list_data,list_remove\n",
    "\n",
    "def product_summary(df,df_item_ref):\n",
    "    print('Start product_summary')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    df_product =df.drop_duplicates(['RSKU_ID','RITEM_DESC'])[['RSKU_ID','RITEM_DESC']]\n",
    "    df_temp = pd.merge(df_product,df_item_ref,left_on=['RSKU_ID'],right_on=['ITEM_IDNT'], how='left')\n",
    "    # df_final = df_temp[['RSKU_ID','RITEM_DESC','REF_NUM']]\n",
    "    df_product_1 = df_temp[['RSKU_ID','RITEM_DESC']]\n",
    "    df_ref_1 = df_temp[['REF_NUM']]\n",
    "    df_ref_1.insert(1,\"REF_DESC\",'')\n",
    "    number_rows_sales = len(df_product)\n",
    "    # writer_excel(df = df_product,path_export_final = path_export_final, cell_export = 'B20',number_sheet = number_sheet,length_start=20 , count_df=number_rows_sales, length_end=116)\n",
    "    dict_data_sku = {'df':df_product_1,'cell_export':'B20'}\n",
    "    dict_data_ref = {'df':df_ref_1,'cell_export':'L20'}\n",
    "    dict_remove = {'count_df':number_rows_sales,'length_start':20,'length_end':116}\n",
    "    list_data.append(dict_data_sku)\n",
    "    list_data.append(dict_data_ref)\n",
    "    list_remove.append(dict_remove)\n",
    "    print('Done product_summary')\n",
    "    return list_data , list_remove\n",
    "\n",
    "def cd_ref(prmt_id,cursor,file_sql,item_code,df_sales, file_sql_2,file_sql_3):\n",
    "    print('Start cd ref')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    df_sales_export = df_sales.drop(['ELI_CLAIM'], axis=1)\n",
    "    print(df_sales_export)\n",
    "    df_ref = connect_sql(cursor=cursor,file_sql = file_sql ,item_code = item_code,var_1 = prmt_id)\n",
    "    df_ref_groupby = df_ref.groupby('CLM_REF_NUM').agg({'CLM_PRODUCT':'sum'}).sort_values(by='CLM_PRODUCT', ascending=True).reset_index()\n",
    "    df_sales_daily = pd.concat([df_sales, df_ref_groupby], axis=1 )\n",
    "    print('Done cd ref')\n",
    "    print('start state ref')\n",
    "    df_state_ref = connect_sql(cursor=cursor,file_sql = file_sql_2 ,item_code = item_code,var_1 = prmt_id)\n",
    "    print('done state ref')\n",
    "    print('start item ref')\n",
    "    df_item_ref = connect_sql(cursor=cursor,file_sql = file_sql_3 ,item_code = item_code,var_1 = prmt_id)\n",
    "    print('done item ref')\n",
    "    # writer_excel(df = df_sales, cell_export = 'B174',number_sheet= str(index_promo)+'_'+str(gst),length_start=174 ,count_df=len(df_sales), length_end=10174,path_export_final=path_export_final)\n",
    "    dict_data_sales = {'df':df_sales_export,'cell_export':'B606'}\n",
    "    dict_data_sales_ref = {'df':df_ref_groupby,'cell_export':'S606'}\n",
    "    dict_remove = {'count_df':len(df_sales),'length_start':606,'length_end':120606}\n",
    "    list_data.append(dict_data_sales)\n",
    "    list_data.append(dict_data_sales_ref)\n",
    "    list_remove.append(dict_remove)\n",
    "    sum_sales = df_sales['ELI_CLAIM'].sum() \n",
    "    if not df_ref_groupby.empty:\n",
    "        sum_ref = df_ref_groupby['CLM_PRODUCT'].sum()\n",
    "    else:\n",
    "        sum_ref = 0\n",
    "    gap_sales_ref = sum_sales - sum_ref\n",
    "    return df_item_ref,df_state_ref,df_ref,df_sales_daily,list_data,list_remove,gap_sales_ref\n",
    "\n",
    "def insert_attachments(sheet_name,file_path_excel,file_path_email,path_export_final):  \n",
    "    print('Start insert email and excel')\n",
    "    print(file_path_excel)\n",
    "    print(file_path_email)\n",
    "    xl = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "    xl.Visible = False\n",
    "    xl.DisplayAlerts = False\n",
    "    wb = xl.Workbooks.Open(path_export_final, UpdateLinks = True)\n",
    "    ws = wb.Worksheets(sheet_name)\n",
    "    try:\n",
    "        excel_name = file_path_excel.split('/')[-1][0:10]\n",
    "    except:\n",
    "        excel_name ='excel'\n",
    "    try:\n",
    "        email_name = file_path_email.split('/')[-1][0:10]\n",
    "    except:\n",
    "        email_name = 'email'\n",
    "    obj = ws.OLEObjects()\n",
    "    xl.DisplayAlerts = False\n",
    "    #xl.AskToUpdateLinks = False\n",
    "    try:\n",
    "        obj.Add(ClassType=None, Filename=file_path_excel, Link=False, DisplayAsIcon=True, IconFileName=iconPath_excel,IconIndex=0, IconLabel = excel_name , Left=ws.Range(\"J8\").Left, Top=ws.Range(\"J8\").Top, Width=50, Height=50)\n",
    "        print(f'Successfully insert excel file in sheet {sheet_name}')\n",
    "    except com_error:\n",
    "        print(f'Cannot insert excel file in sheet {sheet_name}')\n",
    "        pass\n",
    "    try:\n",
    "        obj.Add(ClassType=None, Filename=file_path_email, Link=False, DisplayAsIcon=True, IconFileName=iconPath_email,IconIndex=0, IconLabel = email_name , Left= ws.Range(\"L8\").Left, Top=ws.Range(\"L8\").Top, Width=50, Height=50)\n",
    "        print(f'Successfully insert email file in sheet {sheet_name}')\n",
    "    except com_error:\n",
    "        print(f'Cannot insert email file in sheet {sheet_name}')\n",
    "        pass\n",
    "    xl.DisplayAlerts = True\n",
    "    #xl.AskToUpdateLinks = True\n",
    "    wb.Save()\n",
    "    wb.Close()\n",
    "    # xl.Application.Quit()\n",
    "    #del xl\n",
    "    print('Done insert email and excel')\n",
    "    return None\n",
    "  \n",
    "def main():\n",
    "    cursor = set_up(config = config_coles)\n",
    "    logging.info('-----------------------------------START CHECK COLUMN-----------------------------------------------------')\n",
    "    df_raw_check = connect_sql(cursor = cursor,file_sql = file_sql_claimpack,item_code=month_filter)\n",
    "    logging.info('----------------------------------DONE CHECK COLUMN------------------------------------------------------------------')\n",
    "    #Export checklist\n",
    "    # df_raw_check.to_csv(path_check_list,index=False)\n",
    "    # return 0\n",
    "    # print(df_raw_check)\n",
    "    ###############################################################\n",
    "    error_list =[]\n",
    "    time_start = datetime.datetime.now()\n",
    "    # Filter df_splited with condition , keep TO QA and PRGX\n",
    "    # df_raw_filter = df_raw_check[(df_raw_check['CHECK_COLUMN'] == 'TO QA_STATE')] \n",
    "    df_raw_filter = df_raw_check[df_raw_check['CHECK_COLUMN'].str.contains('TO QA')]\n",
    "    # df_raw_filter = df_raw_check[(df_raw_check['CHECK_COLUMN'] == 'TO QA_NATIONAL')| (df_raw_check['CHECK_COLUMN'] == 'TO QA_STATE')] \n",
    "    df_unique_supp_filter = df_raw_filter[['SUPPLIER','PRMTN_COMP_IDNT','CHECK_COLUMN']].drop_duplicates().values.tolist()\n",
    "    # Create dictionary with supp_num key and list of promo_ids after filter conditions, keep check again and to QA\n",
    "    dict_sup_pro_filter = {}\n",
    "    j=0\n",
    "    for list_sup in df_unique_supp_filter:\n",
    "        if j == 0:\n",
    "            dict_sup_pro_filter[list_sup[0]] = [[list_sup[1]] + [list_sup[2]]]\n",
    "        else:\n",
    "            if list_sup[0] in dict_sup_pro_filter.keys():\n",
    "                dict_sup_pro_filter[list_sup[0]].append([list_sup[1]] + [list_sup[2]])\n",
    "            else:\n",
    "                dict_sup_pro_filter[list_sup[0]] = [[list_sup[1]] + [list_sup[2]]]\n",
    "        j+=1\n",
    "    # return 0\n",
    "    check_list_promo_index = 1\n",
    "    for supp_num,list_pmt_id_classify in dict_sup_pro_filter.items():\n",
    "        index_promo=1\n",
    "        # summary_index = 1\n",
    "        summary_index_list = []\n",
    "        logging.info(f'-------------------------------------------------Working on supp_num {supp_num}-----------------------------------------------------')\n",
    "        for pmt_id_classif in list_pmt_id_classify:\n",
    "            classify_state = pmt_id_classif[1]\n",
    "            pmt_id = pmt_id_classif[0]\n",
    "            print('-------------------------------------------------------------------------------------------------------------------------------------')\n",
    "            check_list_promo = []\n",
    "            df_splited_filter = df_raw_filter[(df_raw_filter['SUPPLIER'] == supp_num) & (df_raw_filter['PRMTN_COMP_IDNT'] == pmt_id)] \n",
    "            # get some important variable\n",
    "            supp_num_list_final,item_list_final,item_input_function = get_info(df_splited = df_splited_filter)\n",
    "            supp_desc = df_splited_filter['SUPP_DESC'].unique()[0].replace(\"/\",\"\")\n",
    "            print('supp_desc',supp_desc)\n",
    "            gst = int(df_splited_filter['CML_COST_GST_RATE_PCT'].unique()[0])\n",
    "            dept = df_splited_filter['DEPT_IDNT'].unique()[0]\n",
    "            dept_desc = df_splited_filter['DEPT_DESC'].unique()[0]\n",
    "            prmt_id = df_splited_filter['PRMTN_COMP_IDNT'].unique()[0]\n",
    "            prmt_name = df_splited_filter['PRMTN_COMP_NAME'].unique()[0] \n",
    "            logging.info(f'----------------------------------------Working on supp_desc {supp_desc}, prmtn id {pmt_id}------------------------------------------')\n",
    "            try:\n",
    "                paf_loc = df_splited_filter['PAF_LOCATION'].unique()[0]\n",
    "            except Exception :\n",
    "                paf_loc = '0'\n",
    "            try:\n",
    "                email_loc = df_splited_filter['EMAIL'].unique()[0]\n",
    "            except Exception :\n",
    "                email_loc = '0'\n",
    "            vendor_num = df_splited_filter['VENDOR_NUM'].unique()[0] \n",
    "            clm_start = df_splited_filter['CLM_START'].unique()[0].astype(str)\n",
    "            clm_end = df_splited_filter['CLM_END'].unique()[0].astype(str)\n",
    "            clm_start_converted = datetime.datetime.strptime(clm_start,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%d/%m/%Y')\n",
    "            clm_end_converted = datetime.datetime.strptime(clm_end,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%d/%m/%Y')\n",
    "            #create path for excel and path_xlsb for excel\n",
    "            path_export_final = path_export+'CS_SCAN_'+supp_desc+'_Analyst_date.xlsx'\n",
    "            path_export_final_xlsb = path_export+'CS_SCAN_'+supp_desc+'_Analyst_date_'+supp_num+'.xlsb'\n",
    "            create_worksheet(index_promo=index_promo,gst=gst,path_export_final=path_export_final)\n",
    "            # if classify_state == 'TO QA_NATIONAL_SP' or classify_state == 'TO QA_NATIONAL_TH' :\n",
    "            if 'NATIONAL' in classify_state:\n",
    "                df_splited_filter = df_splited_filter[['SKU_ID','PROMO_PRICE','CLM_RATE']].drop_duplicates()\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['PROMO_PRICE','CLM_RATE'])['SKU_ID'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['SKU_ID'] = df_splited_filter['SKU_ID'].apply(lambda x : convert_to_input_function(x))\n",
    "                item_list_dict = df_splited_filter.set_index('SKU_ID')[['PROMO_PRICE','CLM_RATE']].to_dict('index')\n",
    "                for key,value in item_list_dict.items():\n",
    "                    item_list_dict[key] = [item_list_dict[key]['PROMO_PRICE']] + [item_list_dict[key]['CLM_RATE']] \n",
    "                print(item_list_dict)\n",
    "            else:\n",
    "                df_splited_filter = df_splited_filter[['SKU_ID','STATE','PROMO_PRICE','CLM_RATE']].drop_duplicates()\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['PROMO_PRICE','CLM_RATE','STATE'])['SKU_ID'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['SKU_ID'] = df_splited_filter['SKU_ID'].apply(lambda x : convert_to_input_function(x))\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['PROMO_PRICE','CLM_RATE','SKU_ID'])['STATE'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['STATE'] = df_splited_filter['STATE'].apply(lambda x : convert_to_input_sql(x))\n",
    "                item_list_dict = df_splited_filter.set_index(['SKU_ID','STATE'])[['PROMO_PRICE','CLM_RATE']].to_dict('index')\n",
    "                for key,value in item_list_dict.items():\n",
    "                    item_list_dict[key] = [item_list_dict[key]['PROMO_PRICE']] + [item_list_dict[key]['CLM_RATE']] \n",
    "                    print(item_list_dict) \n",
    "            # To create excel file\n",
    "            summary_index_list.append(str(index_promo)+'_'+str(gst))\n",
    "            # writer_excel_without_remove_rows(df = df_splited,path = path_export_final,cell_export = 'A1',number_sheet=str(index_promo)+'_'+str(gst),path_export_final=path_export_final)   \n",
    "            df_sales = df_sales_data(cursor = cursor,item_list_dict_gsted = item_list_dict ,start_date = clm_start_converted,end_date = clm_end_converted,classify_state=classify_state)   \n",
    "            # df_ref,df_sales = cd_ref(prmt_id=prmt_id,cursor = cursor,file_sql=file_sql_cd_ref,item_code=item_list_final,df_sales=df_sales)  \n",
    "            df_item_ref,df_state_ref,df_ref,df_sales,list_data_sales,list_remove_sales,gap_sales_ref  = cd_ref(prmt_id=prmt_id,cursor = cursor,file_sql=file_sql_cd_ref,item_code=item_list_final,df_sales=df_sales,file_sql_2 = file_sql_cd_ref_listagg, file_sql_3 = file_sql_cd_ref_listagg_item)  \n",
    "            return df_ref\n",
    "            list_data_state,list_remove_state = product_state_summary(df = df_sales,df_ref=df_state_ref)\n",
    "            list_data_product ,list_remove_product  = product_summary(df = df_sales, df_item_ref = df_item_ref)\n",
    "            dict_data_dept = {'df':dept,'cell_export':'F8'}\n",
    "            dict_data_prmt_id = {'df':prmt_id,'cell_export':'B12'}\n",
    "            dict_data_prmt_name = {'df':prmt_name,'cell_export':'C12'}\n",
    "            dict_data_paf_loc = {'df':paf_loc,'cell_export':'J4'}\n",
    "            dict_data_email_loc = {'df':email_loc,'cell_export':'K4'}\n",
    "            dict_data_supp_num = {'df':supp_num,'cell_export':'E8'}\n",
    "            dict_data_supp_desc = {'df':supp_desc,'cell_export':'C8'}\n",
    "            dict_data_vendor_num = {'df':vendor_num,'cell_export':'D8'}\n",
    "            dict_data_claim_number = {'df':str(index_promo)+'_'+str(gst),'cell_export':'B16'}\n",
    "            list_data = list_data_sales + list_data_state + list_data_product + [dict_data_dept] + [dict_data_prmt_id] + [dict_data_prmt_name] + [dict_data_paf_loc] + [dict_data_email_loc] + [dict_data_supp_num] + [dict_data_supp_desc] + [dict_data_vendor_num] + [dict_data_claim_number]\n",
    "            list_remove = list_remove_sales + list_remove_state + list_remove_product\n",
    "            #  Fill sheet Complete Daily Sales Data\n",
    "            writer_excel(data = list_data, remove = list_remove,number_sheet= str(index_promo)+'_'+str(gst),path_export_final=path_export_final) \n",
    "            try: \n",
    "                insert_attachments(sheet_name = str(index_promo)+'_'+str(gst),file_path_excel = paf_loc ,file_path_email = email_loc,path_export_final = path_export_final)\n",
    "                # logging.warning('---------------------------Cannot insert attachments------------------------------------------------------------')\n",
    "            except:\n",
    "                logging.warning('---------------------------Cannot insert attachments------------------------------------------------------------')\n",
    "            # insert_attachments(sheet_name = str(index_promo)+'_'+str(gst),file_path_excel = paf_loc ,file_path_email = email_loc,path_export_final = path_export_final)\n",
    "            index_promo+=1\n",
    "            check_list_promo_index += 1 \n",
    "            #export check_list_promo\n",
    "            with xw.App(visible=False) as app:\n",
    "                print('check_list_promo_index')\n",
    "                if check_list_promo_index == 2:\n",
    "                    wb = app.books.open('check_list_promo.xlsx')\n",
    "                else:\n",
    "                    wb = app.books.open(path_check_list_promo)\n",
    "                wb_sheet = wb.sheets['Sheet1']\n",
    "                check_list_promo = [supp_num] + [supp_desc] +[prmt_id] + [clm_start] + [clm_end]+ [dept] +[dept_desc]+[gap_sales_ref] + [classify_state] +['Done'] \n",
    "                print(check_list_promo)\n",
    "                wb_sheet.range(f'A{check_list_promo_index}').value =  check_list_promo\n",
    "                wb.save(path_check_list_promo)              \n",
    "        # Fill sheet Vendor Summary\n",
    "        fill_summary_sheet(summary_index_list= summary_index_list,path_export_final=path_export_final)         \n",
    "        remove_sheet_change_xlsb(sheet_name = 'template',path_export_final=path_export_final ,path_export_final_xlsb = path_export_final_xlsb)  \n",
    "        print('-------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(datetime.datetime.now() - time_start)  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if not os.path.isdir(fr\"D:\\\\python\\\\claim_pack_python\\\\claim_qty\\\\{folder_name}\"):\n",
    "    #     os.mkdir(fr\"D:\\\\python\\\\claim_pack_python\\\\claim_qty\\\\{folder_name}\")\n",
    "    df_ref = main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa8299f20a01bb8560ebc0db582b7ed77a2c0ddde0b2c0251417c7c412141a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
