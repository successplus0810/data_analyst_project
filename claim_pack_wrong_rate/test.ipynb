{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "{'970404': [['55564790', 'TO QA_STATE_SP_WRONGRATE']]}\n",
      "-------------------------------------------------Working on supp_num 970404 -----------------------------------------------------\n",
      "---------------------------------------------Working on prmtn_id 55564790 ---------------------------------------------------------\n",
      "970404\n",
      "   PRMTN_TYPE_CDE PRMTN_COMP_IDNT   PRMTN_COMP_NAME PRMTN_COMP_DTL_SDT  \\\n",
      "0              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "1              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "2              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "3              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "4              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "5              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "6              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "7              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "8              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "9              SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "10             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "11             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "12             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "13             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "14             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "15             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "16             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "17             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "18             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "19             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "20             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "21             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "22             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "23             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "24             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "25             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "26             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "27             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "28             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "29             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "30             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "31             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "32             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "33             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "34             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "35             SP        55564790  OCEAN SPRAY 1.5L         2021-12-29   \n",
      "\n",
      "   PRMTN_COMP_DTL_END_DT PRM_IDNT                 PRM_NAME  \\\n",
      "0             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "1             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "2             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "3             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "4             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "5             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "6             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "7             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "8             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "9             2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "10            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "11            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "12            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "13            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "14            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "15            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "16            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "17            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "18            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "19            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "20            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "21            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "22            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "23            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "24            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "25            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "26            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "27            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "28            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "29            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "30            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "31            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "32            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "33            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "34            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "35            2022-01-04    79769  PROMO GRID WK 27 SIMPLE   \n",
      "\n",
      "                   PRM_DESC  PRM_START    PRM_END  ... PVN_CLAIM_NUMBER  \\\n",
      "0   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "1   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "2   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "3   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "4   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "5   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "6   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "7   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "8   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "9   PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "10  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "11  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "12  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "13  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "14  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "15  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "16  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "17  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "18  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "19  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "20  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "21  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "22  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "23  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "24  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "25  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "26  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "27  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "28  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "29  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "30  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "31  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "32  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "33  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "34  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "35  PROMO GRID WK 27 SIMPLE 2021-12-29 2022-12-27  ...             None   \n",
      "\n",
      "   PVN_SUM_CLAIM PRGX_CLAIM_NUMBER PRGX_SUM_AMOUNTDUE FILTER_ITEM  \\\n",
      "0           None              None               None         YES   \n",
      "1           None              None               None         YES   \n",
      "2           None              None               None         YES   \n",
      "3           None              None               None         YES   \n",
      "4           None              None               None         YES   \n",
      "5           None              None               None         YES   \n",
      "6           None              None               None         YES   \n",
      "7           None              None               None         YES   \n",
      "8           None              None               None         YES   \n",
      "9           None              None               None         YES   \n",
      "10          None              None               None         YES   \n",
      "11          None              None               None         YES   \n",
      "12          None              None               None         YES   \n",
      "13          None              None               None         YES   \n",
      "14          None              None               None         YES   \n",
      "15          None              None               None         YES   \n",
      "16          None              None               None         YES   \n",
      "17          None              None               None         YES   \n",
      "18          None              None               None         YES   \n",
      "19          None              None               None         YES   \n",
      "20          None              None               None         YES   \n",
      "21          None              None               None         YES   \n",
      "22          None              None               None         YES   \n",
      "23          None              None               None         YES   \n",
      "24          None              None               None         YES   \n",
      "25          None              None               None         YES   \n",
      "26          None              None               None         YES   \n",
      "27          None              None               None         YES   \n",
      "28          None              None               None         YES   \n",
      "29          None              None               None         YES   \n",
      "30          None              None               None         YES   \n",
      "31          None              None               None         YES   \n",
      "32          None              None               None         YES   \n",
      "33          None              None               None         YES   \n",
      "34          None              None               None         YES   \n",
      "35          None              None               None         YES   \n",
      "\n",
      "   CLM_QTY_ITEM PROMO_QTY_ITEM  SLS_QTY_ALL_ITEM  OUTPUT_QTY  \\\n",
      "0    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "1    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "2    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "3    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "4    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "5    10658.0000     10421.0000        10612.0000  10612.0000   \n",
      "6     3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "7     3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "8     3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "9     3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "10    3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "11    3514.0000      3421.0000         3483.0000   3483.0000   \n",
      "12   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "13   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "14   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "15   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "16   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "17   16302.0000     15968.0000        16205.0000  16205.0000   \n",
      "18    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "19    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "20    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "21    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "22    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "23    7212.0000      7079.0000         7214.0000   7214.0000   \n",
      "24    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "25    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "26    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "27    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "28    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "29    6911.0000      6719.0000         6835.0000   6835.0000   \n",
      "30   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "31   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "32   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "33   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "34   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "35   34739.0000     33952.0000        34529.0000  34529.0000   \n",
      "\n",
      "                CHECK_COLUMN  \n",
      "0   TO QA_STATE_SP_WRONGRATE  \n",
      "1   TO QA_STATE_SP_WRONGRATE  \n",
      "2   TO QA_STATE_SP_WRONGRATE  \n",
      "3   TO QA_STATE_SP_WRONGRATE  \n",
      "4   TO QA_STATE_SP_WRONGRATE  \n",
      "5   TO QA_STATE_SP_WRONGRATE  \n",
      "6   TO QA_STATE_SP_WRONGRATE  \n",
      "7   TO QA_STATE_SP_WRONGRATE  \n",
      "8   TO QA_STATE_SP_WRONGRATE  \n",
      "9   TO QA_STATE_SP_WRONGRATE  \n",
      "10  TO QA_STATE_SP_WRONGRATE  \n",
      "11  TO QA_STATE_SP_WRONGRATE  \n",
      "12  TO QA_STATE_SP_WRONGRATE  \n",
      "13  TO QA_STATE_SP_WRONGRATE  \n",
      "14  TO QA_STATE_SP_WRONGRATE  \n",
      "15  TO QA_STATE_SP_WRONGRATE  \n",
      "16  TO QA_STATE_SP_WRONGRATE  \n",
      "17  TO QA_STATE_SP_WRONGRATE  \n",
      "18  TO QA_STATE_SP_WRONGRATE  \n",
      "19  TO QA_STATE_SP_WRONGRATE  \n",
      "20  TO QA_STATE_SP_WRONGRATE  \n",
      "21  TO QA_STATE_SP_WRONGRATE  \n",
      "22  TO QA_STATE_SP_WRONGRATE  \n",
      "23  TO QA_STATE_SP_WRONGRATE  \n",
      "24  TO QA_STATE_SP_WRONGRATE  \n",
      "25  TO QA_STATE_SP_WRONGRATE  \n",
      "26  TO QA_STATE_SP_WRONGRATE  \n",
      "27  TO QA_STATE_SP_WRONGRATE  \n",
      "28  TO QA_STATE_SP_WRONGRATE  \n",
      "29  TO QA_STATE_SP_WRONGRATE  \n",
      "30  TO QA_STATE_SP_WRONGRATE  \n",
      "31  TO QA_STATE_SP_WRONGRATE  \n",
      "32  TO QA_STATE_SP_WRONGRATE  \n",
      "33  TO QA_STATE_SP_WRONGRATE  \n",
      "34  TO QA_STATE_SP_WRONGRATE  \n",
      "35  TO QA_STATE_SP_WRONGRATE  \n",
      "\n",
      "[36 rows x 66 columns]\n",
      "supp_desc STUART ALEXANDER OCEAN SPRAY\n",
      "{('2521026,4451861,4586598,4950759,7258551,9793057', \"'SA','TAS','VIC'\"): [3.59, Decimal('1.00')], ('4451861', \"'WA'\"): {'UNIT_PRICE_TTL': 3.63, 'MAX_RATE': Decimal('1.00')}, ('2521026,4451861,4586598,4950759,7258551,9793057', \"'NSW','QLD'\"): {'UNIT_PRICE_TTL': 3.64, 'MAX_RATE': Decimal('1.02')}, ('2521026,4586598,4950759,7258551,9793057', \"'WA'\"): {'UNIT_PRICE_TTL': 3.68, 'MAX_RATE': Decimal('1.00')}}\n",
      "{('2521026,4451861,4586598,4950759,7258551,9793057', \"'SA','TAS','VIC'\"): [3.59, Decimal('1.00')], ('4451861', \"'WA'\"): [3.63, Decimal('1.00')], ('2521026,4451861,4586598,4950759,7258551,9793057', \"'NSW','QLD'\"): {'UNIT_PRICE_TTL': 3.64, 'MAX_RATE': Decimal('1.02')}, ('2521026,4586598,4950759,7258551,9793057', \"'WA'\"): {'UNIT_PRICE_TTL': 3.68, 'MAX_RATE': Decimal('1.00')}}\n",
      "{('2521026,4451861,4586598,4950759,7258551,9793057', \"'SA','TAS','VIC'\"): [3.59, Decimal('1.00')], ('4451861', \"'WA'\"): [3.63, Decimal('1.00')], ('2521026,4451861,4586598,4950759,7258551,9793057', \"'NSW','QLD'\"): [3.64, Decimal('1.02')], ('2521026,4586598,4950759,7258551,9793057', \"'WA'\"): {'UNIT_PRICE_TTL': 3.68, 'MAX_RATE': Decimal('1.00')}}\n",
      "{('2521026,4451861,4586598,4950759,7258551,9793057', \"'SA','TAS','VIC'\"): [3.59, Decimal('1.00')], ('4451861', \"'WA'\"): [3.63, Decimal('1.00')], ('2521026,4451861,4586598,4950759,7258551,9793057', \"'NSW','QLD'\"): [3.64, Decimal('1.02')], ('2521026,4586598,4950759,7258551,9793057', \"'WA'\"): [3.68, Decimal('1.00')]}\n",
      "TO QA_STATE_SP_WRONGRATE\n",
      "('2521026,4451861,4586598,4950759,7258551,9793057', \"'SA','TAS','VIC'\") [3.59, Decimal('1.00')]\n",
      "('4451861', \"'WA'\") [3.63, Decimal('1.00')]\n",
      "('2521026,4451861,4586598,4950759,7258551,9793057', \"'NSW','QLD'\") [3.64, Decimal('1.02')]\n",
      "('2521026,4586598,4950759,7258551,9793057', \"'WA'\") [3.68, Decimal('1.00')]\n",
      "------------------------------------------len(df_sales) : 252---------------\n",
      "Start cd ref\n",
      "Done cd ref\n",
      "start state ref\n",
      "done state ref\n",
      "start item ref\n",
      "done item ref\n",
      "79642.580000 79336.0000 306.580000\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import snowflake.connector as sf\n",
    "import os\n",
    "import xlwings as xw\n",
    "from xlwings.constants import DeleteShiftDirection\n",
    "import datetime\n",
    "from pywintypes import com_error\n",
    "import win32com.client as win32\n",
    "import logging\n",
    "import math\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# current_dir = os.getcwd()\n",
    "# os.chdir(r'D:\\python\\claim_pack_python')\n",
    "os.chdir(\"D:\\\\python\\\\claim_pack_wrong_rate\")\n",
    "current_dir = r'D:\\python\\claim_pack_wrong_rate'\n",
    "###### Analyst fill\n",
    "folder_name = '202202'\n",
    "month_filter = '202202'\n",
    "###############################################\n",
    "config_coles = r\"config.json\"\n",
    "\n",
    "file_sql_claimpack = r\"claim_pack_state.sql\"\n",
    "file_sql_summ = r\"summarizer.sql\"\n",
    "file_sql_summ_state = r\"summarizer_state.sql\"\n",
    "file_sql_summ_th = r\"summarizer_th.sql\"\n",
    "file_sql_summ_state_th = r\"summarizer_state_th.sql\"\n",
    "file_sql_cd_ref = r\"cd_ref.sql\"\n",
    "file_sql_cd_check_cole_online = r\"cd_check_cole_online.sql\"\n",
    "file_sql_ven_stop_trading = r\"check_ven_stop_trading.sql\"\n",
    "file_sql_cd_check_prgx = r\"cd_check_prgx.sql\"\n",
    "file_sql_cd_ref_listagg = r\"cd_ref_listagg.sql\"\n",
    "file_sql_cd_ref_listagg_item = r\"cd_ref_listagg_item.sql\"\n",
    "file_sql_check_prof = r\"check_profectus_detail.sql\"\n",
    "\n",
    "path_check_list = fr\"D:\\\\python\\\\claim_pack_wrong_rate\\\\wrong_rate\\\\{folder_name}\\\\checklist.csv\"\n",
    "path_check_list_promo = fr\"D:\\\\python\\\\claim_pack_wrong_rate\\\\wrong_rate\\\\{folder_name}\\\\check_list_promo.xlsx\"\n",
    "\n",
    "path_export = fr\"D:\\\\python\\\\claim_pack_wrong_rate\\\\wrong_rate\\\\{folder_name}\\\\\"\n",
    "path_excel = r\"CS_SCAN_Vendorname_Analyst_Date.xlsx\"\n",
    "path_dna = r\"DNA.xlsx\"\n",
    "path_vba = r'CS_SCAN_vendorname_analyst_yyyymmdd.xlsb'\n",
    "\n",
    "iconPath_email = r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\OUTLOOK.EXE\"\n",
    "iconPath_excel = r\"C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE\"\n",
    "\n",
    "try:\n",
    "    os.remove(r'D:\\python\\claim_pack_wrong_rate\\claim_pack_wrong_rate.log')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(filename=\"claim_pack_scan.log\",\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    filemode='a')\n",
    " \n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    " \n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def set_up(config):\n",
    "    \"\"\"Set up connection to SnowFlake\"\"\"\n",
    "    config = json.loads(open(config).read())\n",
    "    account = config['snowflake']['account']\n",
    "    user = config['snowflake']['user']\n",
    "    warehouse = config['snowflake']['warehouse']\n",
    "    role = config['snowflake']['role']\n",
    "    database = config['snowflake']['database']\n",
    "    schema = config['snowflake']['schema']\n",
    "    password = config['snowflake']['password']\n",
    "    auth = config['snowflake']['authenticator']\n",
    "\n",
    "    conn = sf.connect(user=user, password=password, account=account, authenticator=auth,\n",
    "                      warehouse=warehouse, role=role, database=database, schema=schema)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    return cursor\n",
    "def connect_sql(cursor,file_sql,item_code=0,var_1=0,var_2=0,var_3=0,var_4=0,var_5=''):\n",
    "    try:\n",
    "        # cursor.execute((open(file_sql).read()))\n",
    "        cursor.execute((open(file_sql).read()).format(item_code,var_1,var_2,var_3,var_4,var_5))\n",
    "        all_rows = cursor.fetchall()\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "    finally:\n",
    "        pass\n",
    "        # conn.close()\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    try:\n",
    "        df.columns = field_names\n",
    "    except ValueError:\n",
    "        return pd.DataFrame(columns= field_names)\n",
    "    return df\n",
    "\n",
    "def convert_to_input_sql(num_list):\n",
    "    num_list_final = ''\n",
    "    # print('SUPP LIST',supp_num_list)\n",
    "    for num_list in num_list:\n",
    "        num_list_final = num_list_final + \"'\" + num_list + \"',\"\n",
    "    return num_list_final[:-1]\n",
    "\n",
    "def convert_to_input_function(num_list):\n",
    "    num_list_final = ''\n",
    "    # print('SUPP LIST',supp_num_list)\n",
    "    for num_list in num_list:\n",
    "        num_list_final = num_list_final + num_list + ','\n",
    "    return num_list_final[:-1]\n",
    "\n",
    "def get_info(df_splited):\n",
    "    supp_num_list = list(df_splited['SUPPLIER'].drop_duplicates())\n",
    "    item_list = list(df_splited['SKU_ID'].drop_duplicates())\n",
    "\n",
    "    supp_num_list_final = convert_to_input_sql(num_list = supp_num_list)\n",
    "    item_list_final = convert_to_input_sql(num_list = item_list)\n",
    "    item_input_function = convert_to_input_function(num_list = item_list)\n",
    "    return supp_num_list_final,item_list_final,item_input_function\n",
    "\n",
    "def writer_excel(data,remove,number_sheet,path_export_final):\n",
    "    # data = list_data, remove = list_remove,number_sheet= str(index_promo)+'_'+str(gst),path_export_final=path_export_final\n",
    "    #select sheet\n",
    "    print('Start write excel')\n",
    "    sheet_df_mapping = {number_sheet: data}\n",
    "    sheet_df_remove  = {number_sheet: remove}\n",
    "    # Open Excel in background\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb = app.books.open(path_export_final)\n",
    "        # List of current worksheet names\n",
    "        current_sheets = [sheet.name for sheet in wb.sheets]\n",
    "        # Iterate over sheet/df mapping\n",
    "        # If sheet already exist, overwrite current cotent. Else, add new sheet\n",
    "        for sheet_name in sheet_df_mapping.keys():\n",
    "            if sheet_name in current_sheets:\n",
    "                for df_data in data :\n",
    "                    wb.sheets(sheet_name).range(df_data['cell_export']).options(index=False,header=False).value = df_data['df']\n",
    "            else:\n",
    "                'Name of sheet cannot be found in Excel file, please check again'\n",
    "        for sheet_name in sheet_df_remove.keys():\n",
    "            if sheet_name in current_sheets:\n",
    "                for df_remove in remove :\n",
    "                    # wb.sheets(sheet_name).range(df_cell['cell_export']).options(index=False,header=False).value = df_cell['df']\n",
    "                    length_start = df_remove['length_start'] + df_remove['count_df']\n",
    "                    range_length_to_remove = str(length_start)+':'+ str(df_remove['length_end'])\n",
    "                    wb.sheets(sheet_name).range(range_length_to_remove).api.Delete(DeleteShiftDirection.xlShiftUp)\n",
    "            else:\n",
    "                'Name of sheet cannot be found in Excel file, please check again'\n",
    "        wb.save(path_export_final)\n",
    "    print('Done write excel')\n",
    "    return None\n",
    "\n",
    "def fill_summary_sheet(summary_index_list,path_export_final):\n",
    "    print('Start fill summary sheet')\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb_from = app.books.open(path_export_final)\n",
    "        summary_index = 1\n",
    "        for index in summary_index_list:\n",
    "            wb_from.sheets['Vendor Summary'].range('B'+str(summary_index+10)).value = index\n",
    "            summary_index += 1\n",
    "        length_start = summary_index + 10\n",
    "        range_length_to_remove = str(length_start)+':'+ str(30)\n",
    "        print(range_length_to_remove)\n",
    "        wb_from.sheets('Vendor Summary').range(range_length_to_remove).api.Delete(DeleteShiftDirection.xlShiftUp)         \n",
    "        wb_from.save(path_export_final)\n",
    "    return 'Done fill summary sheet' \n",
    "\n",
    "def create_worksheet(index_promo,gst,path_export_final,classify_state):\n",
    "    # Open Excel in background\n",
    "    with xw.App(visible=False) as app:\n",
    "        if index_promo == 1:\n",
    "            wb_from = app.books.open(path_excel)\n",
    "        else :\n",
    "            wb_from = app.books.open(path_export_final)\n",
    "        if 'wrong' in classify_state.lower():\n",
    "            ws_from = wb_from.sheets['template_wrong']\n",
    "        else:\n",
    "            ws_from = wb_from.sheets['template_entirely']\n",
    "        ws_temp = wb_from.sheets['template_wrong']\n",
    "        ws_from.copy(before=ws_temp, name=str(index_promo)+'_'+str(gst))\n",
    "        wb_from.save(path_export_final)\n",
    "    return 'Done create worksheet'     \n",
    "\n",
    "def remove_sheet_change_xlsb(sheet_name_list,path_export_final,path_export_final_xlsb):\n",
    "    print('Start delete sheet & change to xlsb')\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb = app.books.open(path_export_final)   \n",
    "        for sheet_name in sheet_name_list:        \n",
    "            wb.sheets[sheet_name].delete()\n",
    "        wb.save(path_export_final_xlsb)\n",
    "    try:\n",
    "        os.remove(path_export_final)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return print('Done delete sheet & change to xlsb')\n",
    "\n",
    "# item_code=0,var_1=0,var_2=0,var_3=0,var_4=0\n",
    "\n",
    "def df_sales_data(cursor, item_list_dict_gsted,start_date,end_date,classify_state):\n",
    "    i = 0\n",
    "    print(classify_state)\n",
    "    # if classify_state == 'TO QA_STATE_SP' or  classify_state == 'TO QA_STATE_TH' :\n",
    "    if 'STATE' in classify_state :\n",
    "        for key,value in item_list_dict_gsted.items():\n",
    "            print(key,value)\n",
    "            item_code,state = key\n",
    "            # if classify_state == 'TO QA_STATE_SP':\n",
    "            if '_SP' in classify_state :\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_state ,item_code = item_code,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1],var_5 = state)\n",
    "            else:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_state_th ,item_code = item_code,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1],var_5 = state)\n",
    "            if i == 0:\n",
    "                df_merge = df_each_item\n",
    "            else :\n",
    "                df_merge = pd.concat([df_merge, df_each_item], ignore_index=True)\n",
    "            i+=1\n",
    "        # df_merge['ELI_CLAIM'] = df_merge.RQTY_PROMO * df_merge.SCAN_RATE\n",
    "        df_merge= df_merge.sort_values(by=['RSKU_ID','RDAY_DT','RSTATE'], ascending=True).reset_index(drop=True)\n",
    "    else:\n",
    "        for key,value in item_list_dict_gsted.items():\n",
    "            print(key,value)\n",
    "            # if classify_state == 'TO QA_NATIONAL_SP':\n",
    "            if '_SP' in classify_state:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ ,item_code = key,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1])\n",
    "            else:\n",
    "                df_each_item = connect_sql(cursor,file_sql = file_sql_summ_th ,item_code = key,var_1 = start_date,var_2 =end_date,var_3=value[0],var_4=value[1])\n",
    "            if i == 0:\n",
    "                df_merge = df_each_item\n",
    "            else :\n",
    "                df_merge = pd.concat([df_merge, df_each_item], ignore_index=True)\n",
    "            i+=1\n",
    "        # df_merge['ELI_CLAIM'] = df_merge.RQTY_PROMO * df_merge.SCAN_RATE\n",
    "        df_merge= df_merge.sort_values(by=['RSKU_ID','RDAY_DT','RSTATE'], ascending=True).reset_index(drop=True)\n",
    "    return df_merge\n",
    "\n",
    "def product_state_summary(df,df_ref):\n",
    "    print('Start product_state_summary')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    # Find distict item_code and state\n",
    "    # writer_excel(df,cell_export,length_start,count_df,length_end,number_sheet,path_export_final)\n",
    "    df_temp =df.drop_duplicates(['RSKU_ID','RITEM_DESC','RSTATE'])[['RSKU_ID','RITEM_DESC','RSTATE']]\n",
    "    df_temp_2 = pd.merge(df_temp,df_ref,left_on=['RSKU_ID','RSTATE'],right_on=['ITEM_IDNT','CLM_STATE'], how='left')\n",
    "    df_final = df_temp_2[['RSKU_ID','RITEM_DESC','RSTATE','REF_NUM','CLM_QTY','CLM_RATE','CLM_PRODUCT']]\n",
    "    df_sku_desc = df_final[['RSKU_ID','RITEM_DESC']]\n",
    "    df_state = df_final[['RSTATE']]\n",
    "    df_ref = df_final[['REF_NUM','CLM_QTY','CLM_RATE']]\n",
    "    df_ref.insert(1,\"REF_DESC\",'')\n",
    "    # Calculate number of rows\n",
    "    number_rows_state = len(df_ref)\n",
    "\n",
    "    dict_data_sku = {'df':df_sku_desc,'cell_export':'B217'}\n",
    "    dict_data_state = {'df':df_state,'cell_export':'E217'}\n",
    "    dict_data_remove = {'df':df_ref,'cell_export':'M217'}\n",
    "    dict_remove = {'count_df':number_rows_state,'length_start':217,'length_end':1657}\n",
    "    list_data.append(dict_data_sku)\n",
    "    list_data.append(dict_data_state)\n",
    "    list_data.append(dict_data_remove)\n",
    "    list_remove.append(dict_remove)\n",
    "    print('Done product_state_summary')\n",
    "    return list_data,list_remove\n",
    "\n",
    "def product_summary(df,df_item_ref):\n",
    "    print('Start product_summary')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    df_product =df.drop_duplicates(['RSKU_ID','RITEM_DESC'])[['RSKU_ID','RITEM_DESC']]\n",
    "    df_temp = pd.merge(df_product,df_item_ref,left_on=['RSKU_ID'],right_on=['ITEM_IDNT'], how='left')\n",
    "    # df_final = df_temp[['RSKU_ID','RITEM_DESC','REF_NUM']]\n",
    "    df_product_1 = df_temp[['RSKU_ID','RITEM_DESC']]\n",
    "    df_ref_1 = df_temp[['REF_NUM']]\n",
    "    df_ref_1.insert(1,\"REF_DESC\",'')\n",
    "    number_rows_sales = len(df_product)\n",
    "    # writer_excel(df = df_product,path_export_final = path_export_final, cell_export = 'B20',number_sheet = number_sheet,length_start=20 , count_df=number_rows_sales, length_end=116)\n",
    "    dict_data_sku = {'df':df_product_1,'cell_export':'B20'}\n",
    "    dict_data_ref = {'df':df_ref_1,'cell_export':'L20'}\n",
    "    dict_remove = {'count_df':number_rows_sales,'length_start':20,'length_end':212}\n",
    "    list_data.append(dict_data_sku)\n",
    "    list_data.append(dict_data_ref)\n",
    "    list_remove.append(dict_remove)\n",
    "    print('Done product_summary')\n",
    "    return list_data , list_remove\n",
    "\n",
    "def cd_ref(prmt_id,cursor,file_sql,item_code,df_sales, file_sql_2,file_sql_3,classify_state):\n",
    "    print('Start cd ref')\n",
    "    list_data = []\n",
    "    list_remove = []\n",
    "    # df_sales['ELI_CLAIM'] = df_sales.RQTY_PROMO * df_sales.SCAN_RATE\n",
    "    df_sales_export = df_sales\n",
    "    # print(df_sales_export)\n",
    "    df_ref = connect_sql(cursor=cursor,file_sql = file_sql ,item_code = item_code,var_1 = prmt_id)\n",
    "    df_ref_groupby = df_ref.groupby('CLM_REF_NUM').agg({'CLM_PRODUCT':'sum'}).sort_values(by='CLM_PRODUCT', ascending=True).reset_index()\n",
    "    df_sales_daily = pd.concat([df_sales, df_ref_groupby], axis=1 )\n",
    "    print('Done cd ref')\n",
    "    print('start state ref')\n",
    "    df_state_ref = connect_sql(cursor=cursor,file_sql = file_sql_2 ,item_code = item_code,var_1 = prmt_id)\n",
    "    print('done state ref')\n",
    "    print('start item ref')\n",
    "    df_item_ref = connect_sql(cursor=cursor,file_sql = file_sql_3 ,item_code = item_code,var_1 = prmt_id)\n",
    "    print('done item ref')\n",
    "    # writer_excel(df = df_sales, cell_export = 'B174',number_sheet= str(index_promo)+'_'+str(gst),length_start=174 ,count_df=len(df_sales), length_end=10174,path_export_final=path_export_final)\n",
    "    if 'entirely' in classify_state.lower():\n",
    "        df_sales_export_1 = df_sales_export[['RDAY_DT', 'RSKU_ID', 'RITEM_DESC', 'RSTATE', 'RF_NORM_SELL_PRICE_AMT','RPRM_PRICE', 'RTOTAL_SALES_AMT_MOD', 'RQTY','RF_ACTUAL_SELL_PRICE_AMT_MOD', 'RTOTAL_SALES_AMT_NON', 'RQTY_NON','RF_ACTUAL_SELL_PRICE_AMT_MOD_NON', 'RTOTAL_SALES_AMT_MOD_PROMO','RQTY_PROMO', 'RF_ACTUAL_SELL_PRICE_AMT_MOD_1', 'SCAN_RATE']]\n",
    "        dict_data_sales = {'df':df_sales_export_1,'cell_export':'B1662'}\n",
    "        list_data.append(dict_data_sales)\n",
    "        df_sales['ELI_CLAIM'] = df_sales.RQTY_PROMO * df_sales.SCAN_RATE      \n",
    "    else:\n",
    "        df_sales_export_1 = df_sales_export[['RDAY_DT', 'RSKU_ID', 'RITEM_DESC','RSTATE', 'RF_NORM_SELL_PRICE_AMT', 'RPRM_PRICE']]\n",
    "        df_sales_export_2 = df_sales_export[['RQTY']]\n",
    "        df_sales_export_3 = df_sales_export[['SCAN_RATE']]\n",
    "        dict_data_sales_1 = {'df':df_sales_export_1,'cell_export':'B1662'}\n",
    "        dict_data_sales_2 = {'df':df_sales_export_2,'cell_export':'I1662'}\n",
    "        dict_data_sales_3 = {'df':df_sales_export_3,'cell_export':'Q1662'}\n",
    "        list_data.append(dict_data_sales_1)\n",
    "        list_data.append(dict_data_sales_2)\n",
    "        list_data.append(dict_data_sales_3)\n",
    "        df_sales['ELI_CLAIM'] = df_sales.RQTY * df_sales.SCAN_RATE\n",
    "    dict_data_sales_ref = {'df':df_ref_groupby,'cell_export':'S1662'}\n",
    "    dict_remove = {'count_df':len(df_sales),'length_start':1662,'length_end':121662}\n",
    "    # list_data.append(dict_data_sales)\n",
    "    list_data.append(dict_data_sales_ref)\n",
    "    list_remove.append(dict_remove)\n",
    "    # df_sales.to_csv(r'D:\\OneDrive - Profectus Group\\Desktop\\test.csv')\n",
    "    sum_sales = df_sales['ELI_CLAIM'].sum() \n",
    "    if not df_ref_groupby.empty:\n",
    "        sum_ref = df_ref_groupby['CLM_PRODUCT'].sum()\n",
    "    else:\n",
    "        sum_ref = 0\n",
    "    gap_sales_ref = sum_sales - sum_ref\n",
    "    print(sum_sales,sum_ref,gap_sales_ref)\n",
    "    return df_item_ref,df_state_ref,df_ref,df_sales_daily,list_data,list_remove,gap_sales_ref\n",
    "\n",
    "def insert_attachments(sheet_name,file_path_excel,file_path_email,path_export_final):  \n",
    "    print('Start insert email and excel')\n",
    "    print(file_path_excel)\n",
    "    print(file_path_email)\n",
    "    xl = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "    xl.Visible = False\n",
    "    xl.DisplayAlerts = False\n",
    "    wb = xl.Workbooks.Open(path_export_final, UpdateLinks = True)\n",
    "    ws = wb.Worksheets(sheet_name)\n",
    "    try:\n",
    "        excel_name = file_path_excel.split('/')[-1][0:10]\n",
    "    except:\n",
    "        excel_name ='excel'\n",
    "    try:\n",
    "        email_name = file_path_email.split('/')[-1][0:10]\n",
    "    except:\n",
    "        email_name = 'email'\n",
    "    obj = ws.OLEObjects()\n",
    "    xl.DisplayAlerts = False\n",
    "    #xl.AskToUpdateLinks = False\n",
    "    try:\n",
    "        obj.Add(ClassType=None, Filename=file_path_excel, Link=False, DisplayAsIcon=True, IconFileName=iconPath_excel,IconIndex=0, IconLabel = excel_name , Left=ws.Range(\"J8\").Left, Top=ws.Range(\"J8\").Top, Width=50, Height=50)\n",
    "        print(f'Successfully insert excel file in sheet {sheet_name}')\n",
    "    except com_error:\n",
    "        print(f'Cannot insert excel file in sheet {sheet_name}')\n",
    "        pass\n",
    "    try:\n",
    "        obj.Add(ClassType=None, Filename=file_path_email, Link=False, DisplayAsIcon=True, IconFileName=iconPath_email,IconIndex=0, IconLabel = email_name , Left= ws.Range(\"L8\").Left, Top=ws.Range(\"L8\").Top, Width=50, Height=50)\n",
    "        print(f'Successfully insert email file in sheet {sheet_name}')\n",
    "    except com_error:\n",
    "        print(f'Cannot insert email file in sheet {sheet_name}')\n",
    "        pass\n",
    "    xl.DisplayAlerts = True\n",
    "    #xl.AskToUpdateLinks = True\n",
    "    wb.Save()\n",
    "    wb.Close()\n",
    "    # xl.Application.Quit()\n",
    "    #del xl\n",
    "    print('Done insert email and excel')\n",
    "    return None\n",
    "\n",
    "def move_worksheet_to_vba_template(path_xlsb):\n",
    "    print('start move sheets')\n",
    "    with xw.App(visible=False) as app:\n",
    "        wb1 = app.books.open(path_xlsb)\n",
    "        wb2 = app.books.open(path_vba)\n",
    "        print(wb1.sheet_names)\n",
    "        for sheet_name in wb1.sheet_names:\n",
    "            ws1 = wb1.sheets(sheet_name)\n",
    "            ws1.api.Copy(Before=wb2.sheets('Sheet1').api)\n",
    "        wb2.sheets['Sheet1'].delete()\n",
    "        wb1.close()\n",
    "        wb2.save(path_xlsb)\n",
    "    print('end move sheets')\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    cursor = set_up(config = config_coles)\n",
    "    logging.info('-----------------------------------START CHECK COLUMN-----------------------------------------------------')\n",
    "    df_raw_check = connect_sql(cursor = cursor,file_sql = file_sql_claimpack,item_code=month_filter)\n",
    "    logging.info('----------------------------------DONE CHECK COLUMN------------------------------------------------------------------')\n",
    "    #Export checklist\n",
    "    # df_raw_check.drop(columns = ['FILTER_ITEM']).to_csv(path_check_list,index=False)\n",
    "    # df_raw_check.to_csv(path_check_list,index=False)\n",
    "    # return 0\n",
    "    # print(df_raw_check)\n",
    "    ###############################################################\n",
    "    error_list =[]\n",
    "    time_start = datetime.datetime.now()\n",
    "    # Filter df_splited with condition , keep TO QA and PRGX\n",
    "    # df_raw_filter = df_raw_check[(df_raw_check['CHECK_COLUMN'] == 'TO QA_STATE')] \n",
    "    df_raw_filter = df_raw_check[df_raw_check['CHECK_COLUMN'].str.contains('TO QA')]\n",
    "    # df_raw_filter = df_raw_check[(df_raw_check['CHECK_COLUMN'] == 'TO QA_NATIONAL')| (df_raw_check['CHECK_COLUMN'] == 'TO QA_STATE')] \n",
    "    df_unique_supp_filter = df_raw_filter[['SUPPLIER','PRMTN_COMP_IDNT','CHECK_COLUMN']].drop_duplicates().values.tolist()\n",
    "    # Create dictionary with supp_num key and list of promo_ids after filter conditions, keep check again and to QA\n",
    "    dict_sup_pro_filter = {}\n",
    "    j=0\n",
    "    for list_sup in df_unique_supp_filter:\n",
    "        if j == 0:\n",
    "            dict_sup_pro_filter[list_sup[0]] = [[list_sup[1]] + [list_sup[2]]]\n",
    "        else:\n",
    "            if list_sup[0] in dict_sup_pro_filter.keys():\n",
    "                dict_sup_pro_filter[list_sup[0]].append([list_sup[1]] + [list_sup[2]])\n",
    "            else:\n",
    "                dict_sup_pro_filter[list_sup[0]] = [[list_sup[1]] + [list_sup[2]]]\n",
    "        j+=1\n",
    "    df_raw_filter['UNIT_PRICE_TTL'] = df_raw_filter['UNIT_PRICE_TTL'].astype('float').round(2)\n",
    "    df_raw_filter['ELI_MAX_RATE_CD_SUM'] = df_raw_filter['ELI_MAX_RATE_CD_SUM'].astype('float')\n",
    "    df_raw_filter['ELI_MOD_RATE_CD_SUM'] = df_raw_filter['ELI_MOD_RATE_CD_SUM'].astype('float')\n",
    "    dict_splitted = {}\n",
    "    for key,value in dict_sup_pro_filter.items():\n",
    "        if(len(value) <= 5):\n",
    "            dict_splitted[key] = value\n",
    "        else:\n",
    "            max_index_supplier  = math.ceil(len(value) /5)\n",
    "            for i in range(1,max_index_supplier+1):\n",
    "                dict_splitted[f'{key}_v{i}'] = value[5*(i-1):5*i]\n",
    "    print(dict_splitted)\n",
    "    check_list_promo_index = 1\n",
    "    for supp_num,list_pmt_id_classify in dict_splitted.items():\n",
    "        index_promo=1\n",
    "        # summary_index = 1\n",
    "        summary_index_list = []\n",
    "        logging.info(f'-------------------------------------------------Working on supp_num {supp_num}-----------------------------------------------------')\n",
    "        print(f'-------------------------------------------------Working on supp_num {supp_num} -----------------------------------------------------')\n",
    "        for pmt_id_classif in list_pmt_id_classify:\n",
    "            classify_state = pmt_id_classif[1]\n",
    "            pmt_id = pmt_id_classif[0]\n",
    "            print(f'---------------------------------------------Working on prmtn_id {pmt_id} ---------------------------------------------------------')\n",
    "            check_list_promo = []\n",
    "            supp_num_splitted = supp_num.split('_')[0]\n",
    "            print(supp_num_splitted)\n",
    "            df_splited_filter = df_raw_filter[(df_raw_filter['SUPPLIER'] == supp_num_splitted) & (df_raw_filter['PRMTN_COMP_IDNT'] == pmt_id) & (df_raw_filter['FILTER_ITEM'] == 'YES') ] \n",
    "            # get some important variable\n",
    "            supp_num_list_final,item_list_final,item_input_function = get_info(df_splited = df_splited_filter)\n",
    "            print(df_splited_filter)\n",
    "            supp_desc = df_splited_filter['SUPP_DESC'].unique()[0].replace(\"/\",\"\")\n",
    "            sc = ['.', '>', '<', '!', '?', ',', '[', ']', '(', ')', '&', \"'\", '\"', '+', ':', ';']\n",
    "            supp_desc = ''.join([c for c in supp_desc if c not in sc])\n",
    "            supp_desc = supp_desc.strip()\n",
    "            print('supp_desc',supp_desc)\n",
    "            gst = int(df_splited_filter['CML_COST_GST_RATE_PCT'].unique()[0])\n",
    "            dept = df_splited_filter['DEPT_IDNT'].unique()[0]\n",
    "            dept_desc = df_splited_filter['DEPT_DESC'].unique()[0]\n",
    "            prmt_id = df_splited_filter['PRMTN_COMP_IDNT'].unique()[0]\n",
    "            prmt_name = df_splited_filter['PRMTN_COMP_NAME'].unique()[0] \n",
    "            logging.info(f'----------------------------------------Working on supp_desc {supp_desc}, prmtn id {pmt_id}------------------------------------------')\n",
    "            try:\n",
    "                paf_loc = df_splited_filter['PAF_LOCATION'].unique()[0]\n",
    "            except Exception :\n",
    "                paf_loc = '0'\n",
    "            try:\n",
    "                email_loc = df_splited_filter['EMAIL'].unique()[0]\n",
    "            except Exception :\n",
    "                email_loc = '0'\n",
    "            vendor_num = df_splited_filter['VENDOR_NUM'].unique()[0] \n",
    "            clm_start = df_splited_filter['PRMTN_COMP_DTL_SDT'].unique()[0].astype(str)\n",
    "            clm_end = df_splited_filter['PRMTN_COMP_DTL_END_DT'].unique()[0].astype(str)\n",
    "            clm_start_converted = datetime.datetime.strptime(clm_start,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%d/%m/%Y')\n",
    "            clm_end_converted = datetime.datetime.strptime(clm_end,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%d/%m/%Y')\n",
    "            clm_start_converted_promo = datetime.datetime.strptime(clm_start,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%Y-%m-%d')\n",
    "            clm_end_converted_promo = datetime.datetime.strptime(clm_end,'%Y-%m-%dT%H:%M:%S.000%f').strftime('%Y-%m-%d')\n",
    "            amt_max = df_splited_filter['ELI_MAX_RATE_CD_SUM'].unique()[0].astype(str)\n",
    "            amt_mod = df_splited_filter['ELI_MOD_RATE_CD_SUM'].unique()[0].astype(str)\n",
    "            max_process_date = df_splited_filter['MAX_PROCESS_DTE'].unique()[0]\n",
    "            min_start_date = str(df_splited_filter['MAX_START_DATE'].min())\n",
    "            max_end_date = str(df_splited_filter['MAX_END_DATE'].max())\n",
    "            #create path for excel and path_xlsb for excel\n",
    "            path_export_final = path_export+'CS_SCAN_'+supp_desc+'_Analyst_date.xlsx'\n",
    "            path_export_final_xlsb = path_export+'CS_SCAN_'+supp_desc+'_Analyst_date_'+supp_num+'.xlsb'\n",
    "            # sc = ['.', '>', '<', '!', '?', ',', '[', ']', '(', ')', '&', \"'\", '\"', '+', ':', ';']\n",
    "            # path_export_final = ''.join([c for c in path_export_final if c not in sc])\n",
    "            # path_export_final = path_export_final.strip()\n",
    "            # path_export_final_xlsb = ''.join([c for c in path_export_final_xlsb if c not in sc])\n",
    "            # path_export_final_xlsb = path_export_final_xlsb.strip()\n",
    "            # print('path_export_final',path_export_final)\n",
    "            create_worksheet(index_promo=index_promo,gst=gst,path_export_final=path_export_final,classify_state=classify_state)\n",
    "            # if classify_state == 'TO QA_NATIONAL_SP' or classify_state == 'TO QA_NATIONAL_TH' :\n",
    "            if 'NATIONAL' in classify_state:\n",
    "                # df_splited_filter = df_splited_filter[['SKU_ID','STATE','UNIT_PRICE_TTL','MAX_RATE']].drop_duplicates()\n",
    "                df_splited_filter = df_splited_filter[['SKU_ID','UNIT_PRICE_TTL','MAX_RATE']].drop_duplicates()\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['UNIT_PRICE_TTL','MAX_RATE'])['SKU_ID'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['SKU_ID'] = df_splited_filter['SKU_ID'].apply(lambda x : convert_to_input_function(x))\n",
    "                item_list_dict = df_splited_filter.set_index('SKU_ID')[['UNIT_PRICE_TTL','MAX_RATE']].to_dict('index')\n",
    "                for key,value in item_list_dict.items():\n",
    "                    item_list_dict[key] = [item_list_dict[key]['UNIT_PRICE_TTL']] + [item_list_dict[key]['MAX_RATE']] \n",
    "                print(item_list_dict)\n",
    "            else:\n",
    "                df_splited_filter = df_splited_filter[['SKU_ID','STATE','UNIT_PRICE_TTL','MAX_RATE']].drop_duplicates()\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['UNIT_PRICE_TTL','MAX_RATE','STATE'])['SKU_ID'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['SKU_ID'] = df_splited_filter['SKU_ID'].apply(lambda x : convert_to_input_function(x))\n",
    "                df_splited_filter = df_splited_filter.groupby(by = ['UNIT_PRICE_TTL','MAX_RATE','SKU_ID'])['STATE'].agg(list).to_frame().reset_index()\n",
    "                df_splited_filter['STATE'] = df_splited_filter['STATE'].apply(lambda x : convert_to_input_sql(x))\n",
    "                item_list_dict = df_splited_filter.set_index(['SKU_ID','STATE'])[['UNIT_PRICE_TTL','MAX_RATE']].to_dict('index')\n",
    "                for key,value in item_list_dict.items():\n",
    "                    item_list_dict[key] = [item_list_dict[key]['UNIT_PRICE_TTL']] + [item_list_dict[key]['MAX_RATE']] \n",
    "                    print(item_list_dict) \n",
    "            # To create excel file\n",
    "            summary_index_list.append(str(index_promo)+'_'+str(gst))\n",
    "            # writer_excel_without_remove_rows(df = df_splited,path = path_export_final,cell_export = 'A1',number_sheet=str(index_promo)+'_'+str(gst),path_export_final=path_export_final)   \n",
    "            df_sales = df_sales_data(cursor = cursor,item_list_dict_gsted = item_list_dict ,start_date = clm_start_converted,end_date = clm_end_converted,classify_state=classify_state) \n",
    "            print(f\"------------------------------------------len(df_sales) : {df_sales['RSTATE'].count()}---------------\")  \n",
    "            # df_ref,df_sales = cd_ref(prmt_id=prmt_id,cursor = cursor,file_sql=file_sql_cd_ref,item_code=item_list_final,df_sales=df_sales)  \n",
    "            df_item_ref,df_state_ref,df_ref,df_sales,list_data_sales,list_remove_sales,gap_sales_ref  = cd_ref(prmt_id=prmt_id,cursor = cursor,file_sql=file_sql_cd_ref,item_code=item_list_final,df_sales=df_sales,file_sql_2 = file_sql_cd_ref_listagg, file_sql_3 = file_sql_cd_ref_listagg_item,classify_state=classify_state)  \n",
    "            return df_ref\n",
    "            list_data_state,list_remove_state = product_state_summary(df = df_sales,df_ref=df_state_ref)\n",
    "            list_data_product ,list_remove_product  = product_summary(df = df_sales, df_item_ref = df_item_ref)\n",
    "            dict_data_dept = {'df':dept,'cell_export':'F8'}\n",
    "            dict_data_prmt_id = {'df':prmt_id,'cell_export':'B12'}\n",
    "            dict_data_prmt_name = {'df':prmt_name,'cell_export':'C12'}\n",
    "            # dict_data_paf_loc = {'df':paf_loc,'cell_export':'J4'}\n",
    "            # dict_data_email_loc = {'df':email_loc,'cell_export':'K4'}\n",
    "            dict_data_supp_num = {'df':supp_num_splitted,'cell_export':'E8'}\n",
    "            dict_data_supp_desc = {'df':supp_desc,'cell_export':'C8'}\n",
    "            dict_data_vendor_num = {'df':vendor_num,'cell_export':'D8'}\n",
    "            dict_data_claim_number = {'df':str(index_promo)+'_'+str(gst),'cell_export':'B16'}\n",
    "            list_data = list_data_sales + list_data_state + list_data_product + [dict_data_dept] + [dict_data_prmt_id] + [dict_data_prmt_name]  + [dict_data_supp_num] + [dict_data_supp_desc] + [dict_data_vendor_num] + [dict_data_claim_number]\n",
    "            list_remove = list_remove_sales + list_remove_state + list_remove_product\n",
    "            #  Fill sheet Complete Daily Sales Data\n",
    "            writer_excel(data = list_data, remove = list_remove,number_sheet= str(index_promo)+'_'+str(gst),path_export_final=path_export_final) \n",
    "            # try: \n",
    "            #     insert_attachments(sheet_name = str(index_promo)+'_'+str(gst),file_path_excel = paf_loc ,file_path_email = email_loc,path_export_final = path_export_final)\n",
    "            #     # logging.warning('---------------------------Cannot insert attachments------------------------------------------------------------')\n",
    "            # except:\n",
    "            #     logging.warning('---------------------------Cannot insert attachments------------------------------------------------------------')\n",
    "            # insert_attachments(sheet_name = str(index_promo)+'_'+str(gst),file_path_excel = paf_loc ,file_path_email = email_loc,path_export_final = path_export_final)\n",
    "            #export check_list_promo\n",
    "            index_promo+=1\n",
    "            check_list_promo_index += 1 \n",
    "            with xw.App(visible=False) as app:\n",
    "                print('check_list_promo_index')\n",
    "                if check_list_promo_index == 2:\n",
    "                    wb = app.books.open('check_list_promo.xlsx')\n",
    "                else:\n",
    "                    wb = app.books.open(path_check_list_promo)\n",
    "                wb_sheet = wb.sheets['Sheet1']\n",
    "                check_list_promo = [supp_num] + [supp_desc] +[str(index_promo-1)+'_'+str(gst)] +[prmt_id] + [clm_start_converted_promo] + [clm_end_converted_promo]+ [dept] +[dept_desc] + [amt_max] + [amt_mod] + [max_process_date] + [min_start_date] + [max_end_date] + [gap_sales_ref] + [classify_state] +['Done'] + [email_loc] + [paf_loc]\n",
    "                print(check_list_promo)\n",
    "                wb_sheet.range(f'A{check_list_promo_index}').value =  check_list_promo\n",
    "                wb.save(path_check_list_promo) \n",
    "            # index_promo+=1\n",
    "            # check_list_promo_index += 1    \n",
    "            # return 0          \n",
    "        # Fill sheet Vendor Summary\n",
    "        fill_summary_sheet(summary_index_list= summary_index_list,path_export_final=path_export_final)         \n",
    "        remove_sheet_change_xlsb(sheet_name_list = ['template_wrong','template_entirely'],path_export_final=path_export_final ,path_export_final_xlsb = path_export_final_xlsb)\n",
    "        move_worksheet_to_vba_template(path_xlsb=path_export_final_xlsb)  \n",
    "        print('-------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if not os.path.isdir(fr\"D:\\\\python\\\\claim_pack_wrong_rate\\\\wrong_rate\\\\{folder_name}\"):\n",
    "    #     os.mkdir(fr\"D:\\\\python\\\\claim_pack_wrong_rate\\\\wrong_rate\\\\{folder_name}\")\n",
    "    df_ref = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BC0006132042F, BC0006106274F'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    df_ref_groupby = df_ref.groupby('CLM_REF_NUM').agg({'CLM_PRODUCT':'sum'}).sort_values(by='CLM_PRODUCT', ascending=True).reset_index()\n",
    "    ref_num_list = ', '.join(df_ref_groupby['CLM_REF_NUM'].tolist())\n",
    "except:\n",
    "    ref_num_list = ''\n",
    "ref_num_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa8299f20a01bb8560ebc0db582b7ed77a2c0ddde0b2c0251417c7c412141a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
